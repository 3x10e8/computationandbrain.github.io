
---
layout: post
title: "Homework 3"
date:  Friday, September 20 2019
---


### Due Tuesday, Sept. 24 at 11:59 pm on Gradescope. 

The purpose of this assignment is to highlight connections between neural networks. This assignment involves 1 video lecture, one review article, and one technical paper, so be sure to plan accordingly. Please review all the material and summarize what you find interesting. *You only have to write two pages* but you are welcome to write more.  You do not have to answer every "sub-question."

1. Watch [Geoff Hinton's and Yann Lecun's Turing award speeches](https://www.youtube.com/watch?v=VsnQf7exv5I) (They start at ~10:00 and ~42:00). Summarize the parts of the talk that you find interesting. Feel free to use these questions as a guide:

    1. What do you think of Hinton's "two views of internal representations?" Do you find anything appealing about "symbolic AI?"
    2. What were some of the backpropagation improvements made circa 2005-2009 referenced in the talk? Why are they so important?
    3. What is LeCun's big future research agenda (at Facebook), and do you find it compelling? Why or why not?

2. Read this review paper [Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing](https://www.annualreviews.org/doi/full/10.1146/annurev-vision-082114-035447) (Kriegeskorte 2015) Feel free to use these questions as a guide:
    1. What was HMAX and what were some of its strengths? What were some of its failures?
    2. What is a Helmholtz machine? Why was it so important?
    2. How would you interpret the results of the Khaligh-Razavi et al. paper referenced in the review? (e.g. see Figure 5)
    3. What ideas do you find compelling in the recurrent neural network section?

3. Skim this classic paper "Approximation Capabilities of Multilayer Feedforward Networks" (Hornik 1991) [here](http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf). Feel free to use this question as a guide:
    1. What are the practical implications of this paper?



The following references are optional but encouraged. If you would like to write about these topics instead, please feel free to do so.

* As a reference on CS-style learning theory (touched on in class), see Chapter 20 of [Understanding Machine Learning](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) (the textbook pdf is available online). Focus on the sections covering the expressive power of neural networks and the sample complexity of neural networks. To review PAC Learning see chapter 3.1 and VC dimension in chapter 6.

* As a reference on all things neural networks (especially CNNs), see these phenomenal course notes from Stanford [CS231N](http://cs231n.github.io/convolutional-networks/). 
