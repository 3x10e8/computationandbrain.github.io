---
layout: post
title: "Homework 3"
date:  Friday, September 20 2019
---


### Due Tuesday, Sept. 24 at 11:59 pm on Gradescope. 

The purpose of this assignment is to highlight connections between current neural network research and the brain/cognitive science/neuroscience. This assignment involves 1 video lecture, 1 review article, and 1 technical paper. Please review all the material and summarize what you find interesting. *You only have to write 2 pages* but you are welcome to write more.  Please discuss each of the 3 materials in your summary. You do not have to answer all the "sub-questions."

1. Watch [Geoff Hinton's and Yann Lecun's Turing award speeches](https://www.youtube.com/watch?v=VsnQf7exv5I) (they start at ~10:00 and ~42:00). Summarize the parts of the talk that you find interesting. Feel free to use these questions as a guide:

    1. What do you think of Hinton's "two views of internal representations?" Do you find anything appealing about "symbolic AI?" (a.k.a "old school" AI)
    2. What were some of the backpropagation improvements made circa 2005-2009 referenced in the talk? Why are they so important?
    3. What is a "capsule network," and why is it interesting? (Here is the paper referenced in the talk [Stacked Capsule Autoencoders](https://arxiv.org/pdf/1906.06818.pdf))
    4. Do you share Hinton's cynicysm about reinforcement learning? Do you think NLP puts the nail in the coffin of symbolic AI?
    5. What do you think is LeCun's big future research agenda (at Facebook), and do you find it compelling? Why or why not?
    6. What is "Energy-Based Self-Supervised learning?" (around 1:10:00 mark)
    7. Here is a paper referenced in LeCun's talk about ["Learning Invariant Features through Topographic Filter Maps"](http://yann.lecun.com/exdb/publis/pdf/koray-cvpr-09.pdf). In this paper they learn features such as edge-detecting filters from natural images via an unsupervised algorithm. How is it similar to the Olshausen and Fields 1996 paper from HW2? How is it different?
    7. Do you agree with LeCun about biological inspiration?

2. Read this review paper [Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing](https://www.annualreviews.org/doi/full/10.1146/annurev-vision-082114-035447) (Kriegeskorte 2015) Feel free to use these questions as a guide:
    1. What was HMAX and what were some of its strengths? What were some of its failures?
    2. What is a Helmholtz machine? Why was it so important?
    3. How would you interpret the results of the Khaligh-Razavi et al. paper referenced in the review? (e.g. see Figure 5)
    4. What ideas do you find compelling in the recurrent neural network section?

3. Skim this classic paper "Approximation Capabilities of Multilayer Feedforward Networks" (Hornik 1991) [here](http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf). Feel free to use this question as a guide:
    1. What are the practical implications of this paper?
    2. What does CS theory have to offer "applied" neural network research? What does CS theory have to offer cognitive science/neuroscience?
    
    If you find this paper difficult to read (which we expect will be the case for many of you), please see the wikipedia page on the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem). The Hornik 1991 paper is put into context here. A great "visual proof" of the universal approximation theorem can be found online: <http://neuralnetworksanddeeplearning.com/chap4.html>
    
4. What are two discussion points/questions that you have? (We will choose some of these to discuss in class)

Submit a (preferably LaTeX formatted) summary (~2 pages) to [Gradescope](https://www.gradescope.com/courses/61715).

Thanks, and enjoy!

-----------------------

The following references are optional but encouraged.

* As a reference on CS-style learning theory (touched on in class), see Chapter 20 of [Understanding Machine Learning](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) (the textbook pdf is available online). Focus on the sections covering the expressive power of neural networks and the sample complexity of neural networks. To review PAC Learning see chapter 3.1 and VC dimension in chapter 6.

* As a reference on all things neural networks (especially CNNs), see these phenomenal course notes from Stanford [CS231N](http://cs231n.github.io/convolutional-networks/). 

* Here is the diagram from an Emmanuel Dupoux paper providing a roadmap for cognition inspired AI [Cognitive science in the era of artificial intelligence:
A roadmap for reverse-engineering the infant language-learner](https://arxiv.org/pdf/1607.08723.pdf). LeCun cited his diagram on perception.

![Dupoux]({{ site.url }}/assets/Dupoux.png)

And here is the one on cognition:

![Dupoux]({{ site.url }}/assets/DupouxCognition.png)

------------------------

Here is a diagram that will hopefully clarify what an ON cell is and what an OFF cell is:


![RGC ON OFF]({{ site.url }}/assets/RGCONOFF.jpg)

It was shown already by the early 1950s that retinal ganglion cells (RGCs) gave ~3 types of responses to diffused illumination (Granit 1947, 1955): 'on' units, 'off' units and 'on-off' units. Kuffler (1953) showed that the central and peripheral regions of the receptive fields can be mutually antagonistic (hence center-surround). These findings were extended and confirmed (Barlow, FitzHugh & Kuffler, 1957 a, b; Wiesel, 1958; Hubel, 1960)[[1]](https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1960.sp006557).

It is now known that there are many types of RGCs, that exhibit many types of properties that don't necessarily fall neatly into 'on,' 'off,' or 'on-off.' For example, the paper ["The functional diversity of retinal ganglion cells in the mouse" (Baden et al. 2016)](https://search.proquest.com/docview/1759544312?accountid=10226&pq-origsite=360link) clusters RGC responses into ~20 functional classes across the 'on'-'off' spectrum. However, this framework is still widely used and remains conceptually useful.


![RGC 2016]({{ site.url }}/assets/RGC2016.png)





