
---
layout: post
title: "Homework 3"
date:  Friday, September 20 2019
---


### Due Tuesday, Sept. 24 at 11:59 pm on Gradescope. 

The purpose of this assignment is to highlight connections between current neural network research and the brain/cognitive science/neuroscience. This assignment involves 1 video lecture, 1 review article, and 1 technical paper. Please review all the material and summarize what you find interesting. *You only have to write 2 pages* but you are welcome to write more.  Please discuss each of the 3 materials in your summary. You do not have to answer all the "sub-questions."

1. Watch [Geoff Hinton's and Yann Lecun's Turing award speeches](https://www.youtube.com/watch?v=VsnQf7exv5I) (they start at ~10:00 and ~42:00). Summarize the parts of the talk that you find interesting. Feel free to use these questions as a guide:

    1. What do you think of Hinton's "two views of internal representations?" Do you find anything appealing about "symbolic AI?"
    2. What were some of the backpropagation improvements made circa 2005-2009 referenced in the talk? Why are they so important?
    3. What is LeCun's big future research agenda (at Facebook), and do you find it compelling? Why or why not?

2. Read this review paper [Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing](https://www.annualreviews.org/doi/full/10.1146/annurev-vision-082114-035447) (Kriegeskorte 2015) Feel free to use these questions as a guide:
    1. What was HMAX and what were some of its strengths? What were some of its failures?
    2. What is a Helmholtz machine? Why was it so important?
    2. How would you interpret the results of the Khaligh-Razavi et al. paper referenced in the review? (e.g. see Figure 5)
    3. What ideas do you find compelling in the recurrent neural network section?

3. Skim this classic paper "Approximation Capabilities of Multilayer Feedforward Networks" (Hornik 1991) [here](http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf). Feel free to use this question as a guide:
    1. What are the practical implications of this paper?
    2. What does CS theory have to offer "applied" neural network research? What does CS theory have to offer cognitive science/neuroscience?

-----------------------

The following references are optional but encouraged.

* As a reference on CS-style learning theory (touched on in class), see Chapter 20 of [Understanding Machine Learning](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) (the textbook pdf is available online). Focus on the sections covering the expressive power of neural networks and the sample complexity of neural networks. To review PAC Learning see chapter 3.1 and VC dimension in chapter 6.

* As a reference on all things neural networks (especially CNNs), see these phenomenal course notes from Stanford [CS231N](http://cs231n.github.io/convolutional-networks/). 
