---
layout: post
title: "Homework 5"
date:  Wednesday, October 2 2019
---


### Due Tuesday, October 8 at 11:59 pm on Gradescope. 

NOTE: THIS ASSIGNMENT IS NOT FINALIZED


1. Watch the 2 Jeff Lichtman lectures on "Mapping Neural Connections and Their Development" [[part I]](https://www.youtube.com/watch?v=R2US2yVO4us&t=1s) [[part II]](https://www.youtube.com/watch?v=alIu9NeLbZs). Summarize what you find interesting in this lecture. Respond to at least 2 of the following questions (feel free to respond to more):
    1. Why is/isn't connectomics important for neuroscience?
    2. Do you agree with Lichtman's "U of Science?"
    3. What was the difference between the expected and observed connectivity in the thalamus (part II ~43:00)?
    4. What are some examples of different inhibitory motifs (possibly) exhibited by a single inhibitory neuron (part II ~56:00)?

2. TBD Read ["Random synaptic feedback weights support error backpropagation for deep learning"](https://www.nature.com/articles/ncomms13276.pdf) by Lillicrap et. al. 



3. TBD Skim this paper by Lin, Tegmark and Rolnick [Why Does Deep and Cheap Learning Work So Well?](https://link.springer.com/article/10.1007/s10955-017-1836-5). Answer the following three questions:
    1. Prove the comment given in the paper: the monomial \Pi x_i of n variables can be evaluated using 4n neurons with log_2(n) hidden layers.
    
    2. Prove that a single hidden layer of O(n^d * 2^d) neurons is sufficient to compute any polynomial in n variables of degree d, i.e. polynomials consisting of terms c*\pi x_i^{p_i} where \sum p_i \leq d.
    
    3. C([0,1]^n) is the set of functions of n variables continuous on [0,1]^n. The Stone Weierstrass theorem states the following: let A be a subset of C([0,1]^n). If
        1. A contains a non-zero constant function, and
        2. for any points x \neq y in [0,1]^n there is some function f in A s.t. f(x)\neq f(y) (i.e. it "separates x and y").
Then, for any continuous function f in C([0,1]^n) and for any epsilon, there is an f' in A s.t. f and f' are epsilon-close: for all x in [0,1]^n, |f(x)-f'(x)| \leq epsilon.
Your task: Show that for any epsilon and any continuous function on [0,1]^n (it doesn't even have to be smooth!) there is a neural network with one hidden layer that is epsilon close on all points.



2) Prove that a single hidden layer of O(n^d * 2^d) neurons is sufficient to compute any polynomial in n variables of degree d, i.e. polynomials consisting of terms c*\pi x_i^{p_i} where \sum p_i \leq d. 


4. What are two discussion points/questions that you have? (We will choose some of these to discuss in class)

5. (5 pts) Please elaborate on your thoughts for a potential project in a few sentences. Feel free to reference any of the papers from lectures or from the homeworks. Be prepared to discuss in class

Submit a (preferably LaTeX formatted) summary (~2 pages) to [Gradescope](https://www.gradescope.com/courses/61715). Please sign up as per the instructions on [Piazza](https://piazza.com/columbia/fall2019/comse6998_004_2019_1topicsincomputerscience). 

Thanks, and enjoy!
