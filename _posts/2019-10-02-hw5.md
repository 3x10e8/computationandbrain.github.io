---
layout: post
title: "Homework 5"
date:  Wednesday, October 2 2019
---


### Due Tuesday, October 8 at 11:59 pm on Gradescope. 

NOTE: THIS ASSIGNMENT IS NOT FINALIZED


1. Watch the 2 Jeff Lichtman lectures on "Mapping Neural Connections and Their Development" [[part I]](https://www.youtube.com/watch?v=R2US2yVO4us&t=1s) [[part II]](https://www.youtube.com/watch?v=alIu9NeLbZs). Summarize what you find interesting in this lecture. Respond to at least 2 of the following questions (feel free to respond to more):
    1. Why is/isn't connectomics important for neuroscience?
    2. Do you agree with Lichtman's "U of Science?"
    3. What was the difference between the expected and observed connectivity in the thalamus (part II ~43:00)?
    4. What are some examples of different inhibitory motifs (possibly) exhibited by a single inhibitory neuron (part II ~56:00)?

Choose 2 of the 3 papers below to read and summarize. The 

2. **Paper 1** Read ["Random synaptic feedback weights support error backpropagation for deep learning"](https://www.nature.com/articles/ncomms13276.pdf) by Lillicrap et. al. Answer both of the questions:

    1. First, implement a simple feedword neural network as described on page 3: there are two parameter matrices, W0 and W. W0 maps inputs x to h, and W maps h to y. For this experiment take all vectors to live in R^n with n=10 and generate a random linear map T from R^n -> R^n. Either randomly initialize W0,W or with constants.
For 1000 time steps, generate a "sample" by randomly sampling a unit x in R^n, computing the error, and updating W0, W with gradients scaled by a learning rate of 0.01. Note that the closed form of the gradients are in the section of the paper. Plot the norm of the error which should plummet very quickly.
    2. Now slightly modify your network to use a random B (fixed before the experiment begins) as described in the paper. Plot the norm of the error; what happens? Also, compute the norm of the difference between B and W over time; what happens?



3. **Paper 2:** Read this paper by Lin, Tegmark and Rolnick [Why Does Deep and Cheap Learning Work So Well?](https://link.springer.com/article/10.1007/s10955-017-1836-5). Answer the following three questions:
    1. Prove the comment given in the paper: the monomial \Pi x_i of n variables can be evaluated using 4n neurons with log_2(n) hidden layers.
    
    2. Prove that a single hidden layer of O(n^d * 2^d) neurons is sufficient to compute any polynomial in n variables of degree d, i.e. polynomials consisting of terms c*\pi x_i^{p_i} where \sum p_i \leq d.
    
    3. C([0,1]^n) is the set of functions of n variables continuous on [0,1]^n. The Stone Weierstrass theorem states the following: let A be a subset of C([0,1]^n). If
        1. A contains a non-zero constant function, and
        2. for any points x \neq y in [0,1]^n there is some function f in A s.t. f(x)\neq f(y) (i.e. it "separates x and y") Then, for any continuous function f in C([0,1]^n) and for any epsilon, there is an f' in A s.t. f and f' are epsilon-close: for all x in [0,1]^n, |f(x)-f'(x)| \leq epsilon.

Your task: Show that for any epsilon and any continuous function on [0,1]^n (it doesn't even have to be smooth!) there is a neural network with one hidden layer that is epsilon close on all points.

4. **Paper 3:** Read this paper by Song et al. [Highly Nonrandom Features of Synaptic Connectivity in Local Cortical Circuits](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0030068). Summarize the key take-aways in 1-2 paragraphs.


4. What are two discussion points/questions that you have? (We will choose some of these to discuss in class)

5. (5 pts) Please elaborate on your thoughts for a potential project in a few sentences. Feel free to reference any of the papers from lectures or from the homeworks. Be prepared to discuss in class

Submit a (preferably LaTeX formatted) summary (~2 pages) to [Gradescope](https://www.gradescope.com/courses/61715). Please sign up as per the instructions on [Piazza](https://piazza.com/columbia/fall2019/comse6998_004_2019_1topicsincomputerscience). 

Thanks, and enjoy!
