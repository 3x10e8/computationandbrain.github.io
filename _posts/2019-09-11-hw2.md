---
layout: post
title: "Homework 2"
date:  Wednesday, September 11 2019
---


### Due Tuesday, Sept. 17 at 11:59 pm on Gradescope. 

The purpose of this assignment is to reinforce the introductory material on visual neuroscience from the lecture, and to expose you to some recent research in computational neuroscience. This assignment involves >2 hours of video lectures, so be sure to plan accordingly. Please review all the material and summarize. Unlike last week's homework, you do not have to answer every sub-question.

1. Watch [this talk by Bruno Olshausen](https://simons.berkeley.edu/talks/olshausen-neural) on the Neural Mechanisms of Vision. Summarize the parts of the talk that you find interesting. Be sure to include *some* of the following questions:
    1. What is an "ON" cell and an "OFF" cell?
    2. What is a center-surround?
    3. What was Horace Barlow's important hypothesis i.e. efficient coding?
    4. What is the main result of the "Efficient Coding Model of Retina" (Karklin and Simoncelli 2011) mentioned in the talk? Why do you think this is interesting?
    5. What do you think the "rest" of V1 is doing? (~85%!!)
    6. What do you think of the "Recurrent View" of what the cortex is doing (from Guillery and Sherman) mentioned at the end of the talk? Do you agree with the Rodney Brooks quote?


2. Read the paper ["Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images", by Olshausen and Field](http://www.cns.nyu.edu/~tony/vns/readings/olshausen-field-1996.pdf). Summarize the parts of the paper that you find interesting. Be sure to include *some* of the following questions:
    1. What is a "simple" cell? What is a "complex" cell? Do you think this is a good distinction to make?
    2. Why is this an exciting/celebrated result? 
  

3. Take a break from the paper and watch [this talk by Jack Gallant](https://simons.berkeley.edu/talks/jack-gallant-2-15-18) on "An Accurate Functional Model of Single Neurons in V1, V2 and V4". Summarize the parts of the talk that you find interesting. Be sure to include *some* of the following questions:
    1. What is a spike triggered average? (You might have to look it up - it is an important tool in neuroscience)
    2. A lot of psychology/cognitive science/neuroscience experiments involve synthetic stimuli such as moving gratings. Why might "natural" images be important? How might synthetic images be experimentally useful?
    3. How is this related to the way people think about deep nets?
    4. So...what do you think V4 does?
    

5. What are two discussion points/questions that you have? (We will choose some of these to discuss in class)

Submit a (preferably LaTeX formatted) summary (~2 pages) to [Gradescope](https://www.gradescope.com/courses/61715). Please sign up as per the instructions on [Piazza](https://piazza.com/columbia/fall2019/comse6998_004_2019_1topicsincomputerscience). 

Thanks, and enjoy!

----------
_Here are some **optional** links that you might find interesting. They could be inspiration for final projects:_


This is the Karklin and Simoncelli (NIPS 2011 - not 2012) paper referenced in the Olshausen talk: [Efficient coding of natural images with a population
of noisy Linear-Nonlinear neurons](http://papers.nips.cc/paper/4384-efficient-coding-of-natural-images-with-a-population-of-noisy-linear-nonlinear-neurons.pdf)

From the abstract:
> Efficient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented filters consistent with responses in primary visual cortex. Here we show that an efficient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the firing rate – predicts receptive fields and response nonlinearities similar to those observed in the retina. Specifically, we develop numerical methods for simultaneously learning the linear filters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields filters that are center-surround and nonlinearities that are rectifying. The filters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have filters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal filters are localized and oriented.

A cool ICLR 2017 paper from the Olshausen group can be found here: [EMERGENCE OF FOVEAL IMAGE SAMPLING FROM
LEARNING TO ATTEND IN VISUAL SCENES](https://arxiv.org/pdf/1611.09430.pdf)

From the abstract:
> We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model’s retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.

This ICLR 2019 paper by the Ganguli group at standard is very conceptually satisfying: [A UNIFIED THEORY OF EARLY VISUAL REPRESENTA- TIONS FROM RETINA TO CORTEX THROUGH ANATOMI- CALLY CONSTRAINED DEEP CNNS]

From the abstract:
> The vertebrate visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and for the first time we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple downstream cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortical networks. This result predicts that the retinas of small vertebrates (e.g. salamander, frog) should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system.

The model discussed in the Gallant talk is not published. However his group has a recent bioRxiv paper on a similar model [The DeepTune framework for modeling and
characterizing neurons in visual cortex area V4](https://www.biorxiv.org/content/biorxiv/early/2018/11/09/465534.full.pdf)

From the abstract:
> Deep neural network models have recently been shown to be effective in predicting single neuron responses in primate visual cortex areas V4. Despite their high predictive accuracy, these models are generally difficult to interpret. This limits their applicability in characterizing V4 neuron function. Here, we propose the DeepTune framework as a way to elicit interpretations of deep neural network-based models of single neurons in area V4. V4 is a midtier visual cortical area in the ventral visual pathway. Its functional role is not yet well understood. Using a dataset of recordings of 71 V4 neurons stimulated with thousands of static natural images, we build an ensemble of 18 neural networkbased models per neuron that accurately predict its response given a stimulus image. To interpret and visualize these models, we use a stability criterion to form optimal stimuli (DeepTune images) by pooling the 18 models together. These DeepTune images not only
confirm previous findings on the presence of diverse shape and texture tuning in area V4, but also provide rich, concrete and naturalistic characterization of receptive fields of individual V4 neurons. The population analysis of DeepTune images for 71 neurons reveals how different types of curvature tuning are distributed in V4. In addition, it also suggests strong suppressive tuning for nearly half of the V4 neurons. Though we focus exclusively on the area V4, the DeepTune framework could be applied more generally to enhance the understanding of other visual cortex areas.

"Not the thinker" (https://www.dailymotion.com/video/x278zke)

![pyramidal logic]({{ site.url }}/assets/thinker.png)
